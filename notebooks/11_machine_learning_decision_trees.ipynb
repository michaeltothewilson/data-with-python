{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21a7ed2-6146-4b20-926c-ce17966e8992",
   "metadata": {},
   "source": [
    "# Machine Learning - Decision Tree - Malignant or Benign?\n",
    "\n",
    "<center><img src=\"../images/generated/Gemini_Generated_Image_8fu90a8fu90a8fu9.jpeg\" width=\"400\"></center>\n",
    "</br>\n",
    "</br>\n",
    "In this activity we will explore a medical diagnosis dataset and apply a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0195e-a3c6-4a6a-84fe-e693c8f60def",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "As usual, we will import a bunch of libraries to get started.\n",
    "\n",
    "note - we will be importing additional libraries later on in the activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b6fef6-753f-4049-8642-82302ddc28f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Begin Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "## End Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d8c01c-72bd-4029-af7f-0983258261a5",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The dataset for this activity is the Scikit-learn breast cancer dataset.\n",
    "\n",
    "The goal is to predict if a breast tumor is malignant (cancerous) or benign (non-cancerous) based on features extracted from a digitized image of a fine needle aspirate (FNA) of a breast mass.\n",
    "\n",
    "(_AI Generated Summary Begins Here_)\n",
    "Data Set Summary:\n",
    "\n",
    "* __Number of Samples:__ 569 tumor samples.\n",
    "\n",
    "* __Features:__ 30 numerical features that describe the characteristics of the cell nuclei in each image. Examples include the mean radius, texture, perimeter, and area.\n",
    "\n",
    "* __Target:__ The target variable is the diagnosis, with two possible classes:\n",
    "\n",
    "* __`0`:__ Malignant (212 samples)\n",
    "\n",
    "* __`1`:__ Benign (357 samples)\n",
    "\n",
    "(_AI Generated Summary Ends Here - Credit Gemini_)\n",
    "\n",
    "__Loading the Dataset:__\n",
    "* `load_breast_cancer()` returns a dictionary containing the data we will use\n",
    "* `data` - an array of feature values\n",
    "* `target` - an array of target values where `0` is `malignant` and `1` is `benign`\n",
    "* `feature_names` - column names\n",
    "* `target_names` - an array of the target names\n",
    "\n",
    "We will load the individual the following items into a DataFrame for inspection:\n",
    "\n",
    "* data\n",
    "* feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e957e25-7b2e-4ea5-af07-1cdf8f5a00b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "breast_cancer = load_breast_cancer()\n",
    "data = breast_cancer[\"data\"]\n",
    "columns = breast_cancer[\"feature_names\"]\n",
    "target = breast_cancer[\"target\"]\n",
    "target_columns = breast_cancer[\"target_names\"]\n",
    "\n",
    "# Load Data Into DataFrame\n",
    "X = pd.DataFrame(data=data, \n",
    "                 columns=columns)\n",
    "\n",
    "y = pd.Series(data=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f5007-ab32-4fb9-b52e-e7844ad16b4e",
   "metadata": {},
   "source": [
    "## Getting Familiar With the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517807ae-4ad8-487c-927b-bb61f4123519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View X's Info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d02c97c-8d9f-4e21-83ae-04d012d1d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview X's Rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f1393-6797-40f5-b90e-f533b4995416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Target Info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0a490b-b2ee-4cb6-8285-93a67f27bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Target Value Counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d120e5a-04c7-4655-be86-a0ef017a5963",
   "metadata": {},
   "source": [
    "## Inspecting the Shape of the Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1147ec1c-c362-4560-850b-53b6b856d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Feature (X) Shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf99ea-e87e-4100-9a9b-62026e7eecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Target (y) Shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e94c45b-4116-4ec8-8a29-9cd301fb2530",
   "metadata": {},
   "source": [
    "## Visualizing the Distribution of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a61ee-058f-4148-8563-1f3cd7cb31ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize Distribution of Feature Data\n",
    "X.hist(figsize=(12,7),\n",
    "       bins=30,\n",
    "      edgecolor=\"black\")\n",
    "plt.subplots_adjust(hspace=0.7,\n",
    "                    wspace=0.4)\n",
    "plt.title(\"Distribution of Feature Data\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dc5bd4-6088-4df5-a81c-d082797b2c8f",
   "metadata": {},
   "source": [
    "## Visualizing Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4056b61-63ae-40e9-a4db-a39940c6f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart of Total Counts of Malignant and Benign\n",
    "y.value_counts().plot(kind=\"bar\",\n",
    "            figsize=(10,5))\n",
    "\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Diagnosis\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.title(\"Tumor Classification Counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659aba89-3510-41ae-ace4-9b4e7318f0f5",
   "metadata": {},
   "source": [
    "## Splitting up the Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb6a6a-6442-48c0-bac6-8723de3f1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Up Data\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a48ba-84bc-4dfd-a99f-9d766df85f30",
   "metadata": {},
   "source": [
    "## Inspecting the Shape and Features of the Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a332707-a9d8-4973-90fe-e304dfa319e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X Train Shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a745f-168b-458a-a5ee-47986eccd08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X Test Shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74660e6e-5e87-4b45-84f6-d4c811c94d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y Train Shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203a8d8-3937-4dbd-8a3d-d2c387888851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y Test Shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f82d6-8a3d-4901-b4dc-cb62085c7d16",
   "metadata": {},
   "source": [
    "## Build Model - Decision Tree\n",
    "\n",
    "Importing Decision Tree Classifier\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21a89e-217e-4bea-98f4-be69920b27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import and Build Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f678f17-38b0-47dd-83d3-54577f3038f9",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc908e78-099b-4b6d-9666-4939e2a68ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0d062-d87e-4be1-850e-8b5d94c139a4",
   "metadata": {},
   "source": [
    "## Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fecf03-c864-44bb-8b3b-3cf1cc9df773",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fbdb9d-56f7-4eea-ae20-f0f0493be448",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Given the nature of the dataset, let's run a few different tests of the model:\n",
    "* Accuracy Score\n",
    "* Confusion Matrix\n",
    "* Precision Score\n",
    "* Recall Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655de37-b03e-4eb3-8d69-8d530b7ec3dd",
   "metadata": {},
   "source": [
    "### Accuracy Score\n",
    "\n",
    "Accuracy Score is the ration of correct predictions to the the total number of predictions.\n",
    "\n",
    "`accuracy_score` takes two arguments:\n",
    "* actual correct labels\n",
    "* predicted labels from model\n",
    "\n",
    "Returns the accuracy as a fraction (float between `0` and `1`)\n",
    "\n",
    "\n",
    "Gets the ratio of correct predictions to the total number of predictions\n",
    "Accuracy = Correct Predictions / Total Number of Predictions\n",
    "\n",
    "__Accuracy Score Pros and Cons__:\n",
    "\n",
    "* (+) Simple and Easy to use\n",
    "* (-) Can be misleading with imbalanced datasets where the numbers in one class is significantly higher than in another\n",
    "\n",
    "__Importing Accuracy Score__:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c7a09-0f57-4ecc-99a2-0bbe93f5ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accuracy Score Results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a59a17-3ff3-437d-a6b2-cfa5d64fbab6",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Best used for binary classification (think spam not spam or in this case malignant or benign)\n",
    "\n",
    "`confusion_matrix()` takes in `y_test` data, model `predictions`, and labels and returns a 2 by 2 tables with 4 key components:\n",
    "\n",
    "* `True Positive` - Model correctly predicted positive\n",
    "* `True Negative` - Model correction predicted negative\n",
    "* `False Positive` - Model incorrectly predicted positive \n",
    "* `False Negative` - Model incorrectly predicted negative\n",
    "\n",
    "__Importing Confusion Matrix__:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee023f-92e3-43d2-9a80-f56aa7fee679",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Confusion Matrix Results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc8e37-43f0-4135-b1fb-b19da98a4b39",
   "metadata": {},
   "source": [
    "#### Confusion Matrix - Making Sense of It\n",
    "\n",
    "Here is a break down of the Confusion Matrix as a table.\n",
    "\n",
    "| |Predicted Negative |Predicted Positive|\n",
    "|:---|:---|:---|\n",
    "|Actual Negative| True Negatives |   False Positives|\n",
    "|Actual Positive |False Negatives  |True Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9929799f-10b5-43c0-ae6a-4d56ae615fd6",
   "metadata": {},
   "source": [
    "### Precision Score\n",
    "Precision measures the accuracy of positive predictions made by a model: \"Of all the instances the model predicted as positive, how many where actually positive?\"\n",
    "\n",
    "* `precision_score` takes in the `y_test` and `predictions` results and returns a float from 0-1.\n",
    "* A higher score indicates higher precision\n",
    "\n",
    "__Formula__:\n",
    "\n",
    "Precision = True Positives / (True Positives + False Positives)\n",
    "\n",
    "\n",
    "__Uses__:\n",
    "\n",
    "Precision is important when the cost of a false positive is high.\n",
    "\n",
    "* Spam Detections\n",
    "* Medical Diagnosis\n",
    "* E-commerce Recommendations\n",
    "\n",
    "__Importing Precision Score__:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_score\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92533e02-b273-4219-bf8c-5aae227ad2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Precision Score Results\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c58dc93-e63f-41bb-9fa6-1ae7c20cfc32",
   "metadata": {},
   "source": [
    "### Recall Score\n",
    "\n",
    "Recall Score asks the question: \"of all the instances that were actually positive, how many did the model correctly identify?\"\n",
    "\n",
    "Focusing on the model's ability to find all the positive cases\n",
    "\n",
    "__Formula:__\n",
    "\n",
    "Recall = True Positive / (True Positive + False Negative)\n",
    "\n",
    "__Uses:__\n",
    "\n",
    "* Recall is important when the the cost of a false negative is high.\n",
    "* For example, medical diagnosis a false negative could have life-threatning consequences.\n",
    "\n",
    "__Importing Recall Score:__\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import recall_score\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4cee2a-dd38-4da9-9935-fe1dc94a3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Recall Score Results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333e878-1d76-4303-a94b-5cb7249c70af",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "\n",
    "Alternatively, can can run `classification_report` to evaluate the model's performance.\n",
    "\n",
    "`classification_report` takes in the `y_test`, `predictions`, and `target_names` and returns:\n",
    "* __Precision Score__ - positive predictions that where correct\n",
    "* __Recall Score__ - how many positive cases the model correctly identified\n",
    "* __F1-Score__ - harmonic mean of precision and recall - ranging from 0-1, 1 is best possible score\n",
    "* __Support__ - number of actual occurences of each class\n",
    "\n",
    "__note:__ a high F1 for a class with low support may not be as meangingful as a slightly lower F1 for a class with high support\n",
    "\n",
    "__Importing Classification Report:__\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757da9a-59b5-43d8-a58c-aa5420a92406",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classification Report Results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e5b37-078f-4a09-8b8f-896b12d4bca4",
   "metadata": {},
   "source": [
    "## Visualizing the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e91b4-1b3f-4048-a473-b58f3ebb5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "plt.figure(figsize=(25,20))\n",
    "tree.plot_tree(clf,\n",
    "               feature_names=df.columns,\n",
    "               class_names={0: \"Malignant\", 1: \"Benign\"},\n",
    "               filled=True,\n",
    "               fontsize=12)\n",
    "\n",
    "plt.title(\"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae0a0c-6f38-4790-af42-2206c394b783",
   "metadata": {},
   "source": [
    "# Improving the Model\n",
    "\n",
    "Next up, you will improve the model.\n",
    "\n",
    "We'll take a look at Feature Importances, prune features that aren't pulliing their weights, rebuild the model, and evaluate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4c3dd-8a6d-4803-a1b2-1da139c5ba7b",
   "metadata": {},
   "source": [
    "### Feature Importances\n",
    "\n",
    "Feature Importances is a technique that measures how much each feature in the dataset contributes to a model's predictions.\n",
    "\n",
    "* Each input variable is given a score that indicates its relative infuence on the model's output.\n",
    "* A higher score means a feature has a larger impact on the model's ability to make accurate predictions.\n",
    "\n",
    "__Access Feature Importance:__\n",
    "\n",
    "```python\n",
    "clf.feature_importances_\n",
    "```\n",
    "\n",
    "In the cell below, let's create a new DataFrame from the `feature_importances` attribute where the index is the columns from the original DataFrame and the column is called `importance`.\n",
    "\n",
    "We will then display the first 10 rows of data in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339f766-26fd-4939-8c5d-239d00fb79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement Feature Importance\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412dada4-fa18-46cc-a7ce-ded303976049",
   "metadata": {},
   "source": [
    "## Visual Feature Importance\n",
    "\n",
    "In the cell below, create a bar graph, visualizing Feature Importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae32404-32ae-4002-b46f-3ee14ebf4366",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize Feature Importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aaba0b-446c-478b-96c6-048d857a4fb0",
   "metadata": {},
   "source": [
    "## Preserve the top 10 features\n",
    "\n",
    "In the cell below, create a new feature DataFrame to be used for training/testing called `X_pruned` that keeps only the top 10 features and view it's information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1abee5-f44a-4a93-a355-95009d5548bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve the top 10 features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34f8e1-212e-4f98-ac48-ecc51c8df503",
   "metadata": {},
   "source": [
    "## Split the New Training Data\n",
    "\n",
    "Now let's split the Data for testing using the `X_pruned` and `y` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ec18b-ea95-4100-88e0-80579ff18628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Split the New Test Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcb6738-923b-43e1-9c85-db77937ecb6c",
   "metadata": {},
   "source": [
    "## Build A New Classifier\n",
    "\n",
    "Create a New Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc4886-f9f7-478c-a7b9-3bfdcfe14ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the New Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87587046-3bb9-4496-a1d1-845d1f7c318e",
   "metadata": {},
   "source": [
    "## Train the New Model\n",
    "\n",
    "Using `X_pruned_train` and `y_train`, train the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b2a210-85b5-44fc-bf6a-b18508251482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the New Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5878b1cb-2b28-4fe1-a742-28778cc0c176",
   "metadata": {},
   "source": [
    "## Run Predictions\n",
    "\n",
    "Using `X_pruned_test`, store the model's predictions results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04988078-3652-4602-9ab5-1c4289a9d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d97c9-a29f-4709-9e8d-dc8cb3f03b03",
   "metadata": {},
   "source": [
    "## Classification Report\n",
    "\n",
    "Run the classification report on the model's predictions.\n",
    "\n",
    "Was there an improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220abc42-effb-47f8-bc68-4c333a30a293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get Classification Report\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
